{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn.metrics import classification_report \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import mlflow.tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>signup_time</th>\n",
       "      <th>purchase_time</th>\n",
       "      <th>purchase_value</th>\n",
       "      <th>device_id</th>\n",
       "      <th>source</th>\n",
       "      <th>browser</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>ip_address</th>\n",
       "      <th>class</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>247547</td>\n",
       "      <td>2015-06-28 03:00:34</td>\n",
       "      <td>2015-08-09 03:57:29</td>\n",
       "      <td>47</td>\n",
       "      <td>KIXYSVCHIPQBR</td>\n",
       "      <td>SEO</td>\n",
       "      <td>Safari</td>\n",
       "      <td>F</td>\n",
       "      <td>30</td>\n",
       "      <td>16778864</td>\n",
       "      <td>0</td>\n",
       "      <td>Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220737</td>\n",
       "      <td>2015-01-28 14:21:11</td>\n",
       "      <td>2015-02-11 20:28:28</td>\n",
       "      <td>15</td>\n",
       "      <td>PKYOWQKWGJNJI</td>\n",
       "      <td>SEO</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>F</td>\n",
       "      <td>34</td>\n",
       "      <td>16842045</td>\n",
       "      <td>0</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>390400</td>\n",
       "      <td>2015-03-19 20:49:09</td>\n",
       "      <td>2015-04-11 23:41:23</td>\n",
       "      <td>44</td>\n",
       "      <td>LVCSXLISZHVUO</td>\n",
       "      <td>Ads</td>\n",
       "      <td>IE</td>\n",
       "      <td>M</td>\n",
       "      <td>29</td>\n",
       "      <td>16843656</td>\n",
       "      <td>0</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69592</td>\n",
       "      <td>2015-02-24 06:11:57</td>\n",
       "      <td>2015-05-23 16:40:14</td>\n",
       "      <td>55</td>\n",
       "      <td>UHAUHNXXUADJE</td>\n",
       "      <td>Direct</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>F</td>\n",
       "      <td>30</td>\n",
       "      <td>16938732</td>\n",
       "      <td>0</td>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>174987</td>\n",
       "      <td>2015-07-07 12:58:11</td>\n",
       "      <td>2015-11-03 04:04:30</td>\n",
       "      <td>51</td>\n",
       "      <td>XPGPMOHIDRMGE</td>\n",
       "      <td>SEO</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>F</td>\n",
       "      <td>37</td>\n",
       "      <td>16971984</td>\n",
       "      <td>0</td>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id          signup_time        purchase_time  purchase_value  \\\n",
       "0   247547  2015-06-28 03:00:34  2015-08-09 03:57:29              47   \n",
       "1   220737  2015-01-28 14:21:11  2015-02-11 20:28:28              15   \n",
       "2   390400  2015-03-19 20:49:09  2015-04-11 23:41:23              44   \n",
       "3    69592  2015-02-24 06:11:57  2015-05-23 16:40:14              55   \n",
       "4   174987  2015-07-07 12:58:11  2015-11-03 04:04:30              51   \n",
       "\n",
       "       device_id  source browser sex  age  ip_address  class    country  \n",
       "0  KIXYSVCHIPQBR     SEO  Safari   F   30    16778864      0  Australia  \n",
       "1  PKYOWQKWGJNJI     SEO  Chrome   F   34    16842045      0   Thailand  \n",
       "2  LVCSXLISZHVUO     Ads      IE   M   29    16843656      0      China  \n",
       "3  UHAUHNXXUADJE  Direct  Chrome   F   30    16938732      0      China  \n",
       "4  XPGPMOHIDRMGE     SEO  Chrome   F   37    16971984      0   Thailand  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fraud = pd.read_csv('../data/Merged_Fraud_Data.csv')\n",
    "df_fraud.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    116878\n",
       "1     12268\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fraud['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   purchase_value       age\n",
      "0        0.549607 -0.363124\n",
      "1       -1.197335  0.101168\n",
      "2        0.385831 -0.479197\n",
      "3        0.986342 -0.363124\n",
      "4        0.767974  0.449387\n"
     ]
    }
   ],
   "source": [
    "##Normalize & Scale Numerical Features\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Select numerical features for scaling\n",
    "num_features = [\"purchase_value\",'age']\n",
    "\n",
    "# Apply StandardScaler (mean = 0, std = 1)\n",
    "scaler = StandardScaler()\n",
    "df_fraud[num_features] = scaler.fit_transform(df_fraud[num_features])\n",
    "\n",
    "# Display scaled values\n",
    "print(df_fraud[num_features].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id          signup_time        purchase_time  purchase_value  \\\n",
      "0   247547  2015-06-28 03:00:34  2015-08-09 03:57:29              47   \n",
      "1   220737  2015-01-28 14:21:11  2015-02-11 20:28:28              15   \n",
      "2   390400  2015-03-19 20:49:09  2015-04-11 23:41:23              44   \n",
      "3    69592  2015-02-24 06:11:57  2015-05-23 16:40:14              55   \n",
      "4   174987  2015-07-07 12:58:11  2015-11-03 04:04:30              51   \n",
      "\n",
      "   device_id  age  ip_address  class  country  source_Direct  source_SEO  \\\n",
      "0      46780   30    16778864      0        7          False        True   \n",
      "1      70073   34    16842045      0      162          False        True   \n",
      "2      53448   29    16843656      0       36          False       False   \n",
      "3      92195   30    16938732      0       36           True       False   \n",
      "4     107434   37    16971984      0      162          False        True   \n",
      "\n",
      "   sex_M  browser_FireFox  browser_IE  browser_Opera  browser_Safari  \n",
      "0  False            False       False          False            True  \n",
      "1  False            False       False          False           False  \n",
      "2   True            False        True          False           False  \n",
      "3  False            False       False          False           False  \n",
      "4  False            False       False          False           False  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Features to One-Hot Encode (only low-cardinality ones)\n",
    "low_cardinality_features = [\"source\", \"sex\", \"browser\"]  # Avoid \"device_id\"\n",
    "\n",
    "# Features to Label Encode (high-cardinality ones)\n",
    "high_cardinality_features = [\"device_id\", \"country\"]\n",
    "\n",
    "# One-Hot Encode only low-cardinality categorical variables\n",
    "df_fraud = pd.get_dummies(df_fraud, columns=low_cardinality_features, drop_first=True)\n",
    "\n",
    "# Label Encode high-cardinality features\n",
    "label_encoder = LabelEncoder()\n",
    "for feature in high_cardinality_features:\n",
    "    if feature in df_fraud.columns:\n",
    "        df_fraud[feature] = label_encoder.fit_transform(df_fraud[feature])\n",
    "\n",
    "# Display transformed data\n",
    "print(df_fraud.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:218\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 218\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(left, right)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:131\u001b[0m, in \u001b[0;36m_evaluate_numexpr\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m     result \u001b[38;5;241m=\u001b[39m _evaluate_standard(op, op_str, a, b)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     72\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m df_fraud[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mip_txn_count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_fraud\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mip_address\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mip_address\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Compute time difference between signup and purchase (in hours)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df_fraud[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignup_to_purchase_hrs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (df_fraud[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpurchase_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m df_fraud[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignup_time\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtotal_seconds() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3600\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Display new features\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_fraud[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_txn_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_txn_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mip_txn_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignup_to_purchase_hrs\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda\\Lib\\site-packages\\pandas\\core\\arraylike.py:194\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arith_method(other, operator\u001b[38;5;241m.\u001b[39msub)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda\\Lib\\site-packages\\pandas\\core\\series.py:6135\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[0;32m   6134\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[1;32m-> 6135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base\u001b[38;5;241m.\u001b[39mIndexOpsMixin\u001b[38;5;241m.\u001b[39m_arith_method(\u001b[38;5;28mself\u001b[39m, other, op)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda\\Lib\\site-packages\\pandas\\core\\base.py:1382\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1382\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:283\u001b[0m, in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    279\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m--> 283\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(left, right, op)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:227\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    221\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m    222\u001b[0m     ):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m         result \u001b[38;5;241m=\u001b[39m _masked_arith_op(left, right, op)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:163\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[1;34m(x, y, op)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 163\u001b[0m         result[mask] \u001b[38;5;241m=\u001b[39m op(xrav[mask], yrav[mask])\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(y):\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute transaction count per user, device, and IP\n",
    "df_fraud[\"user_txn_count\"] = df_fraud.groupby(\"user_id\")[\"user_id\"].transform(\"count\")\n",
    "df_fraud[\"device_txn_count\"] = df_fraud.groupby(\"device_id\")[\"device_id\"].transform(\"count\")\n",
    "df_fraud[\"ip_txn_count\"] = df_fraud.groupby(\"ip_address\")[\"ip_address\"].transform(\"count\")\n",
    "\n",
    "# Compute time difference between signup and purchase (in hours)\n",
    "df_fraud[\"signup_to_purchase_hrs\"] = (df_fraud[\"purchase_time\"] - df_fraud[\"signup_time\"]).dt.total_seconds() / 3600\n",
    "\n",
    "# Display new features\n",
    "print(df_fraud[[\"user_txn_count\", \"device_txn_count\", \"ip_txn_count\", \"signup_to_purchase_hrs\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hour and day of week from purchase_time\n",
    "df_fraud[\"hour_of_day\"] = df_fraud[\"purchase_time\"].dt.hour\n",
    "df_fraud[\"day_of_week\"] = df_fraud[\"purchase_time\"].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "\n",
    "# Display new time features\n",
    "print(df_fraud[[\"hour_of_day\", \"day_of_week\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Splitting the data into Features and Targets\n",
    "X = df_fraud.drop(columns='class',  axis=1)\n",
    "Y = df_fraud['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split the data into Traning data and Testing Data\n",
    "\n",
    "#X_train, X_test, Y_train,Y_test = train_test_split(X,Y, test_size=0.2, stratify=Y, random_state=2)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the column is in datetime format\n",
    "X_train['signup_time'] = pd.to_datetime(X_train['signup_time'])\n",
    "X_test['signup_time'] = pd.to_datetime(X_test['signup_time'])\n",
    "\n",
    "# Convert to Unix timestamp (seconds since epoch)\n",
    "X_train['signup_time'] = X_train['signup_time'].astype('int64') // 10**9\n",
    "X_test['signup_time'] = X_test['signup_time'].astype('int64') // 10**9\n",
    "\n",
    "\n",
    "# Ensure the column is in datetime format\n",
    "X_train['purchase_time'] = pd.to_datetime(X_train['purchase_time'])\n",
    "X_test['purchase_time'] = pd.to_datetime(X_test['purchase_time'])\n",
    "\n",
    "# Convert to Unix timestamp (seconds since epoch)\n",
    "X_train['purchase_time'] = X_train['purchase_time'].astype('int64') // 10**9\n",
    "X_test['purchase_time'] = X_test['purchase_time'].astype('int64') // 10**9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['year'] = pd.to_datetime(X_train['purchase_time']).dt.year\n",
    "X_train['month'] = pd.to_datetime(X_train['purchase_time']).dt.month\n",
    "X_train['day'] = pd.to_datetime(X_train['purchase_time']).dt.day\n",
    "X_train['hour'] = pd.to_datetime(X_train['purchase_time']).dt.hour\n",
    "\n",
    "X_test['year'] = pd.to_datetime(X_test['purchase_time']).dt.year\n",
    "X_test['month'] = pd.to_datetime(X_test['purchase_time']).dt.month\n",
    "X_test['day'] = pd.to_datetime(X_test['purchase_time']).dt.day\n",
    "X_test['hour'] = pd.to_datetime(X_test['purchase_time']).dt.hour\n",
    "\n",
    "X_train['year'] = pd.to_datetime(X_train['signup_time']).dt.year\n",
    "X_train['month'] = pd.to_datetime(X_train['signup_time']).dt.month\n",
    "X_train['day'] = pd.to_datetime(X_train['signup_time']).dt.day\n",
    "X_train['hour'] = pd.to_datetime(X_train['signup_time']).dt.hour\n",
    "\n",
    "X_test['year'] = pd.to_datetime(X_test['signup_time']).dt.year\n",
    "X_test['month'] = pd.to_datetime(X_test['signup_time']).dt.month\n",
    "X_test['day'] = pd.to_datetime(X_test['signup_time']).dt.day\n",
    "X_test['hour'] = pd.to_datetime(X_test['signup_time']).dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     23376\n",
      "           1       0.00      0.00      0.00      2454\n",
      "\n",
      "    accuracy                           0.90     25830\n",
      "   macro avg       0.45      0.50      0.48     25830\n",
      "weighted avg       0.82      0.90      0.86     25830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the model parameters \n",
    "params = {\n",
    "    'solver': 'lbfgs',\n",
    "    'max_iter': 1000,\n",
    "    'multi_class': 'auto',\n",
    "    'random_state': 8888,\n",
    "}\n",
    "\n",
    "#Train the model \n",
    "lr = LogisticRegression(**params)\n",
    "lr.fit(X_train, Y_train)\n",
    "\n",
    "#predict on the test set \n",
    "Y_pred = lr.predict(X_test)\n",
    "\n",
    "report = classification_report(Y_test,Y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9049941927990709,\n",
       "  'recall': 1.0,\n",
       "  'f1-score': 0.950128033166687,\n",
       "  'support': 23376.0},\n",
       " '1': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 2454.0},\n",
       " 'accuracy': 0.9049941927990709,\n",
       " 'macro avg': {'precision': 0.45249709639953545,\n",
       "  'recall': 0.5,\n",
       "  'f1-score': 0.4750640165833435,\n",
       "  'support': 25830.0},\n",
       " 'weighted avg': {'precision': 0.8190144890000419,\n",
       "  'recall': 0.9049941927990709,\n",
       "  'f1-score': 0.8598603524314548,\n",
       "  'support': 25830.0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_dict = classification_report(Y_test, Y_pred, output_dict=True)\n",
    "report_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1: Train Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     23376\n",
      "           1       0.00      0.00      0.00      2454\n",
      "\n",
      "    accuracy                           0.90     25830\n",
      "   macro avg       0.45      0.50      0.48     25830\n",
      "weighted avg       0.82      0.90      0.86     25830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(C=1, solver='liblinear')\n",
    "log_reg.fit(X_train,Y_train)\n",
    "Y_pred_log_reg = log_reg.predict(X_test)\n",
    "print(classification_report(Y_test,Y_pred_log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2: Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     23376\n",
      "           1       0.00      0.00      0.00      2454\n",
      "\n",
      "    accuracy                           0.90     25830\n",
      "   macro avg       0.45      0.50      0.48     25830\n",
      "weighted avg       0.82      0.90      0.86     25830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=30, max_depth=3)\n",
    "rf_clf.fit(X_train,Y_train)\n",
    "Y_pred_rf = rf_clf.predict(X_test)\n",
    "print(classification_report(Y_test,Y_pred_log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 3: Train XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     23376\n",
      "           1       0.00      0.00      0.00      2454\n",
      "\n",
      "    accuracy                           0.90     25830\n",
      "   macro avg       0.45      0.50      0.48     25830\n",
      "weighted avg       0.82      0.90      0.86     25830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = XGBClassifier(use_lable_encoder=False, eval_metric='logloss')\n",
    "xgb_clf.fit(X_train,Y_train)\n",
    "Y_pred_xgb = xgb_clf.predict(X_test)\n",
    "print(classification_report(Y_test,Y_pred_log_reg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 4: Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98     23376\n",
      "           1       0.96      0.55      0.70      2454\n",
      "\n",
      "    accuracy                           0.95     25830\n",
      "   macro avg       0.96      0.77      0.84     25830\n",
      "weighted avg       0.95      0.95      0.95     25830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(max_depth=5)  # You can tune hyperparameters\n",
    "dt_clf.fit(X_train, Y_train)\n",
    "Y_pred_dt = dt_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, Y_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Track ML models Experiments Using MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\n",
    "        'Logistic Regression',\n",
    "        LogisticRegression(C=1, solver='liblinear'),\n",
    "        (X_train,Y_train),\n",
    "        (X_test,Y_test)\n",
    "\n",
    "    ),\n",
    "    (\n",
    "        'Random Forest',\n",
    "        RandomForestClassifier(n_estimators=30, max_depth=3),\n",
    "        (X_train,Y_train),\n",
    "        (X_test,Y_test)\n",
    "    ),\n",
    "    (\n",
    "         'XGBClassifier',\n",
    "          XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "          (X_train,Y_train),\n",
    "          (X_test,Y_test)\n",
    "    ),\n",
    "    (\n",
    "        'Decision Tree',\n",
    "         DecisionTreeClassifier(max_depth=5),\n",
    "        (X_train,Y_train),\n",
    "        (X_test,Y_test)\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = []\n",
    "\n",
    "for model_name, model, train_set, test_set in models:\n",
    "    X_train = train_set[0]\n",
    "    Y_train = train_set[1]\n",
    "    X_test = test_set[0]\n",
    "    Y_test = test_set[1]\n",
    "\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    report = classification_report(Y_test, Y_pred,output_dict=True)\n",
    "    reports.append(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/16 12:50:47 INFO mlflow.tracking.fluent: Experiment with name 'model tranning for fraud_data' does not exist. Creating a new experiment.\n",
      "2025/02/16 12:51:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Logistic Regression at: http://127.0.0.1:5000/#/experiments/200134848443818606/runs/1518b25792294a828563c90c9825081e\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/200134848443818606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/16 12:51:28 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Random Forest at: http://127.0.0.1:5000/#/experiments/200134848443818606/runs/ddd42c9e63914393b2906b644fab5a36\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/200134848443818606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/16 12:51:47 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run XGBClassifier at: http://127.0.0.1:5000/#/experiments/200134848443818606/runs/8b6e960e80a14b6c8bb85379aaa42f06\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/200134848443818606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/16 12:52:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Decision Tree at: http://127.0.0.1:5000/#/experiments/200134848443818606/runs/fcbd5cafdd2c49e5a11d96aec7b30729\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/200134848443818606\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment('model tranning for fraud_data')\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "for i, element in enumerate(models):\n",
    "    model_name = element[0]\n",
    "    model = element[1]\n",
    "    report = reports[i]\n",
    "\n",
    "    with mlflow.start_run(run_name = model_name):\n",
    "        mlflow.log_param('model_name', model_name)\n",
    "        mlflow.log_metric('accuracy', report['accuracy'])\n",
    "        mlflow.log_metric('recall_class_1', report['1']['recall'])\n",
    "        mlflow.log_metric('recall_class_0', report['0']['recall'])\n",
    "        mlflow.log_metric('f1_score_macro', report['macro avg']['f1-score'])\n",
    "\n",
    "        if 'XGB' in model_name:\n",
    "            mlflow.xgboost.log_model(model, 'model')\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(model, 'model')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train  Deep learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1:Multi-Layer Perceptron (MLP) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     23376\n",
      "           1       0.00      0.00      0.00      2454\n",
      "\n",
      "    accuracy                           0.90     25830\n",
      "   macro avg       0.45      0.50      0.48     25830\n",
      "weighted avg       0.82      0.90      0.86     25830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "mlp_clf.fit(X_train, Y_train)\n",
    "Y_pred_mlp = mlp_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test, Y_pred_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 2 : Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - accuracy: 0.8289 - loss: 1866403.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9ms/step - accuracy: 0.8253 - loss: 456893.4375\n",
      "Epoch 3/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 10ms/step - accuracy: 0.8303 - loss: 219493.8906\n",
      "Epoch 4/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11ms/step - accuracy: 0.8249 - loss: 114394.9766\n",
      "Epoch 5/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - accuracy: 0.8478 - loss: 33536.3516\n",
      "Epoch 6/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 11ms/step - accuracy: 0.8979 - loss: 10.8453\n",
      "Epoch 7/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9ms/step - accuracy: 0.9051 - loss: 0.3283\n",
      "Epoch 8/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 10ms/step - accuracy: 0.9058 - loss: 0.3121\n",
      "Epoch 9/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11ms/step - accuracy: 0.9044 - loss: 0.3154\n",
      "Epoch 10/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - accuracy: 0.9053 - loss: 0.3132\n",
      "\u001b[1m808/808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     23376\n",
      "           1       0.00      0.00      0.00      2454\n",
      "\n",
      "    accuracy                           0.90     25830\n",
      "   macro avg       0.45      0.50      0.48     25830\n",
      "weighted avg       0.82      0.90      0.86     25830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# Convert DataFrame to NumPy array\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Reshape input for CNN\n",
    "X_train_cnn = X_train.reshape(-1, X_train.shape[1], 1)\n",
    "X_test_cnn = X_test.reshape(-1, X_test.shape[1], 1)\n",
    "\n",
    "X_train_cnn = X_train_cnn.astype(np.float32)\n",
    "X_test_cnn = X_test_cnn.astype(np.float32)\n",
    "\n",
    "\n",
    "# Check if it's binary or multi-class\n",
    "num_classes = len(np.unique(Y_train))\n",
    "\n",
    "if num_classes == 2:\n",
    "    # Binary classification (0/1)\n",
    "    Y_train_cnn = np.array(Y_train)\n",
    "    Y_test_cnn = np.array(Y_test)\n",
    "    loss_function = \"binary_crossentropy\"\n",
    "    final_activation = \"sigmoid\"\n",
    "    output_units = 1\n",
    "else:\n",
    "    # Multi-class classification\n",
    "    Y_train_cnn = to_categorical(Y_train, num_classes)\n",
    "    Y_test_cnn = to_categorical(Y_test, num_classes)\n",
    "    loss_function = \"categorical_crossentropy\"\n",
    "    final_activation = \"softmax\"\n",
    "    output_units = num_classes\n",
    "\n",
    "# Define CNN model\n",
    "cnn_model = models.Sequential([\n",
    "    layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(output_units, activation=final_activation)  # Adjust for binary/multi-class\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "cnn_model.fit(X_train_cnn, Y_train_cnn, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Predictions\n",
    "Y_pred_cnn = cnn_model.predict(X_test_cnn)\n",
    "Y_pred_cnn_classes = (Y_pred_cnn > 0.5).astype(int) if num_classes == 2 else Y_pred_cnn.argmax(axis=1)\n",
    "\n",
    "print(classification_report(Y_test, Y_pred_cnn_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 7:Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 8ms/step - accuracy: 0.8302 - loss: 32598.8457\n",
      "Epoch 2/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 11ms/step - accuracy: 0.8362 - loss: 73.9595\n",
      "Epoch 3/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 11ms/step - accuracy: 0.8160 - loss: 30.7390\n",
      "Epoch 4/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 11ms/step - accuracy: 0.8364 - loss: 2.0003\n",
      "Epoch 5/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11ms/step - accuracy: 0.8851 - loss: 5.1223\n",
      "Epoch 6/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11ms/step - accuracy: 0.8389 - loss: 2.8985\n",
      "Epoch 7/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - accuracy: 0.8652 - loss: 0.7916\n",
      "Epoch 8/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - accuracy: 0.9032 - loss: 0.3504\n",
      "Epoch 9/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - accuracy: 0.9056 - loss: 0.3198\n",
      "Epoch 10/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - accuracy: 0.9031 - loss: 0.5513\n",
      "\u001b[1m808/808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     23376\n",
      "           1       0.00      0.00      0.00      2454\n",
      "\n",
      "    accuracy                           0.90     25830\n",
      "   macro avg       0.45      0.50      0.48     25830\n",
      "weighted avg       0.82      0.90      0.86     25830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "# # Convert DataFrame to NumPy array\n",
    "\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "\n",
    "Y_train = np.array(Y_train, dtype=np.int32)\n",
    "Y_test = np.array(Y_test, dtype=np.int32)\n",
    "\n",
    "\n",
    "rnn_model = models.Sequential([\n",
    "    SimpleRNN(50, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(output_units, activation=final_activation)  # Adjusted for binary/multi-class\n",
    "])\n",
    "\n",
    "rnn_model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "rnn_model.fit(X_train, Y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "Y_pred_rnn = rnn_model.predict(X_test)\n",
    "Y_pred_rnn_classes = (Y_pred_rnn > 0.5).astype(int) if num_classes == 2 else Y_pred_rnn.argmax(axis=1)\n",
    "\n",
    "print(classification_report(Y_test, Y_pred_rnn_classes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 8:Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 16ms/step - accuracy: 0.8010 - loss: 432141.8438\n",
      "Epoch 2/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 18ms/step - accuracy: 0.8280 - loss: 57401.3125\n",
      "Epoch 3/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 17ms/step - accuracy: 0.8267 - loss: 57181.1602\n",
      "Epoch 4/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 16ms/step - accuracy: 0.8267 - loss: 41752.3047\n",
      "Epoch 5/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 16ms/step - accuracy: 0.8520 - loss: 2774.8015\n",
      "Epoch 6/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 16ms/step - accuracy: 0.9053 - loss: 0.3135\n",
      "Epoch 7/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 16ms/step - accuracy: 0.9050 - loss: 0.3142\n",
      "Epoch 8/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 16ms/step - accuracy: 0.9031 - loss: 0.3187\n",
      "Epoch 9/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 16ms/step - accuracy: 0.9024 - loss: 9.1530\n",
      "Epoch 10/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 17ms/step - accuracy: 0.9046 - loss: 0.3232\n",
      "\u001b[1m808/808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     23376\n",
      "           1       0.00      0.00      0.00      2454\n",
      "\n",
      "    accuracy                           0.90     25830\n",
      "   macro avg       0.45      0.50      0.48     25830\n",
      "weighted avg       0.82      0.90      0.86     25830\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# # Convert DataFrame to NumPy array\n",
    "\n",
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "X_test = np.array(X_test, dtype=np.float32)\n",
    "\n",
    "Y_train = np.array(Y_train, dtype=np.int32)\n",
    "Y_test = np.array(Y_test, dtype=np.int32)\n",
    "\n",
    "lstm_model = models.Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(output_units, activation=final_activation)  # Adjusted for binary/multi-class\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "lstm_model.fit(X_train_cnn, Y_train_cnn, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "Y_pred_lstm = lstm_model.predict(X_test_cnn)\n",
    "Y_pred_lstm_classes = (Y_pred_lstm > 0.5).astype(int) if num_classes == 2 else Y_pred_lstm.argmax(axis=1)\n",
    "\n",
    "print(classification_report(Y_test, Y_pred_lstm_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Track Deep learning models Experiments Using MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_learning_models = {\n",
    "    \"CNN\": models.Sequential([\n",
    "        layers.Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "        layers.MaxPooling1D(pool_size=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(50, activation='relu'),\n",
    "        layers.Dense(output_units, activation=final_activation)\n",
    "    ]),\n",
    "    \"RNN\": models.Sequential([\n",
    "        layers.SimpleRNN(50, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "        layers.Dense(50, activation='relu'),\n",
    "        layers.Dense(output_units, activation=final_activation)\n",
    "    ]),\n",
    "    \"LSTM\": models.Sequential([\n",
    "        layers.LSTM(50, activation='relu', input_shape=(X_train.shape[1], 1)),\n",
    "        layers.Dense(50, activation='relu'),\n",
    "        layers.Dense(output_units, activation=final_activation)\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment('model tranning for fraud_data')\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 12ms/step - accuracy: 0.8305 - loss: 2024370.8750\n",
      "Epoch 2/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 12ms/step - accuracy: 0.8273 - loss: 682612.3750\n",
      "Epoch 3/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 13ms/step - accuracy: 0.8284 - loss: 303261.2812\n",
      "Epoch 4/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - accuracy: 0.8286 - loss: 103756.9219\n",
      "Epoch 5/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 10ms/step - accuracy: 0.8279 - loss: 38138.3555\n",
      "Epoch 6/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11ms/step - accuracy: 0.9026 - loss: 131.0138\n",
      "Epoch 7/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - accuracy: 0.9042 - loss: 0.3159\n",
      "Epoch 8/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - accuracy: 0.9064 - loss: 0.3107\n",
      "Epoch 9/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - accuracy: 0.9044 - loss: 0.3153\n",
      "Epoch 10/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 10ms/step - accuracy: 0.9038 - loss: 0.3166\n",
      "\u001b[1m808/808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/16 13:35:59 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2025/02/16 13:36:29 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged CNN successfully in MLflow\n",
      "🏃 View run CNN at: http://127.0.0.1:5000/#/experiments/200134848443818606/runs/e67592a1c7884b2faf5fd08d8aeecfe8\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/200134848443818606\n",
      "Epoch 1/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 9ms/step - accuracy: 0.8188 - loss: 12467.9385\n",
      "Epoch 2/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10ms/step - accuracy: 0.8257 - loss: 198.6428\n",
      "Epoch 3/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10ms/step - accuracy: 0.8310 - loss: 24.4148\n",
      "Epoch 4/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11ms/step - accuracy: 0.8458 - loss: 1.6816\n",
      "Epoch 5/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 11ms/step - accuracy: 0.8943 - loss: 0.9636\n",
      "Epoch 6/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 12ms/step - accuracy: 0.8950 - loss: 0.4410\n",
      "Epoch 7/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 12ms/step - accuracy: 0.8999 - loss: 0.5920\n",
      "Epoch 8/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 11ms/step - accuracy: 0.9040 - loss: 0.3399\n",
      "Epoch 9/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11ms/step - accuracy: 0.9049 - loss: 0.3183\n",
      "Epoch 10/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11ms/step - accuracy: 0.9041 - loss: 0.3204\n",
      "\u001b[1m808/808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/16 13:42:29 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2025/02/16 13:43:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged RNN successfully in MLflow\n",
      "🏃 View run RNN at: http://127.0.0.1:5000/#/experiments/200134848443818606/runs/7ca7d1eeddec4daaab4acec65ae98c27\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/200134848443818606\n",
      "Epoch 1/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - accuracy: 0.7937 - loss: 391872.7500\n",
      "Epoch 2/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - accuracy: 0.8307 - loss: 24.4156\n",
      "Epoch 3/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - accuracy: 0.8593 - loss: 13515.5186\n",
      "Epoch 4/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - accuracy: 0.8311 - loss: 907.5781\n",
      "Epoch 5/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - accuracy: 0.8525 - loss: 148.7984\n",
      "Epoch 6/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - accuracy: 0.8813 - loss: 851.8013\n",
      "Epoch 7/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 8ms/step - accuracy: 0.9023 - loss: 0.6637\n",
      "Epoch 8/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8ms/step - accuracy: 0.9047 - loss: 0.3148\n",
      "Epoch 9/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 7ms/step - accuracy: 0.9040 - loss: 1.0732\n",
      "Epoch 10/10\n",
      "\u001b[1m3229/3229\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 8ms/step - accuracy: 0.9053 - loss: 0.3134\n",
      "\u001b[1m808/808\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/16 13:47:21 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
      "2025/02/16 13:47:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged LSTM successfully in MLflow\n",
      "🏃 View run LSTM at: http://127.0.0.1:5000/#/experiments/200134848443818606/runs/2ba9f4ce4379409f9fcb9686a2e6e11c\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/200134848443818606\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in deep_learning_models.items():\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Compile and Train\n",
    "        model.compile(optimizer='adam', loss=loss_function, metrics=['accuracy'])\n",
    "        model.fit(X_train_cnn, Y_train_cnn, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "        # Make predictions\n",
    "        Y_pred = model.predict(X_test_cnn)\n",
    "        Y_pred_classes = (Y_pred > 0.5).astype(int) if num_classes == 2 else Y_pred.argmax(axis=1)\n",
    "\n",
    "        # Compute classification metrics\n",
    "        report = classification_report(Y_test, Y_pred_classes, output_dict=True)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            'accuracy': report['accuracy'],\n",
    "            'recall_class_0': report['0']['recall'],\n",
    "            'recall_class_1': report['1']['recall'],\n",
    "            'f1_score_macro': report['macro avg']['f1-score']\n",
    "        })\n",
    "\n",
    "        # Log deep learning model\n",
    "        mlflow.tensorflow.log_model(model, model_name)\n",
    "\n",
    "        print(f\"Logged {model_name} successfully in MLflow\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
